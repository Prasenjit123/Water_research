# -*- coding: utf-8 -*-
"""Another copy of river_data_analysis_with_validation_data_v2(UP)(org)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10JjiHeKOyHInZOtx9kXD9tH6yoAKb9eL
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from keras.models import Sequential
from keras.layers import Conv1D, Flatten, Dense,Dropout, Activation
from keras.layers import SimpleRNN, LSTM, Dense,GRU,Bidirectional
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from sklearn.neural_network import MLPRegressor
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from scipy.stats import pearsonr

# Load the datta
df = pd.read_csv('/content/Ganga River_updated.csv')
df

# Filter rows for each station
row = df[df['Station']=="Uttar Pradesh"]

row

row.describe()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Identify outliers using a boxplot
plt.figure(figsize=(6, 4))
sns.boxplot(data=row[['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']])
plt.title('Boxplot of Selected Features')
plt.show()

# Initialize a dictionary to store the count of outliers for each column
outliers_count = {}

# Iterate through each column in the selected features
for column in ['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']:
    # Calculate quartiles and IQR
    q1 = row[column].quantile(0.25)
    q3 = row[column].quantile(0.75)
    iqr = q3 - q1
    # Define lower and upper bounds for outliers
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    # Identify outliers and replace them with the mean
    outliers = (row[column] < lower_bound) | (row[column] > upper_bound)
    mean_value = row.loc[~outliers, column].mean()
    row.loc[outliers, column] = mean_value
    # Count the number of outliers
    outliers_count[column] = outliers.sum()

# Print the number of outliers detected for each feature
for column, count in outliers_count.items():
    print(f'Number of outliers in {column}: {count}')

# Verify changes
plt.figure(figsize=(6, 4))
sns.boxplot(data=row[['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']])
plt.title('Boxplot after Handling Outliers')
plt.show()

print(row)

#correlation
df_new=df.drop(['Station','Year'],axis=1)
df_new.corr(method='pearson')

# Create train test split (UttarPradesh)
X_train = row[row['Year'].isin([2013,2014,2015,2016,2017,2018])][['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']]
X_val = row[row['Year'].isin([2019])][['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']]
X_test = row[row['Year']==2020][['Temperature (C)','pH','Conductivity (?Siemens/cm)','BOD (mg/L)']]

y_train = row[row['Year'].isin([2013,2014,2015,2016,2017,2018])]['D.O. (mg/L)']
y_val = row[row['Year'].isin([2019])]['D.O. (mg/L)']
y_test = row[row['Year']==2020]['D.O. (mg/L)']

print(X_train.shape)
print(X_val.shape)
print(X_test.shape)

"""MULTIVARIATE LINEAR REGRESSION"""

#LINEAR REGRESSION
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = y_train   # no Scaling
y_val_scaled = y_val     # no scaling
y_test_scaled = y_test     # no scaling

# Linear Regression Model
LR_model = LinearRegression()
LR_model.fit(X_train_scaled, y_train_scaled)
# Predictions on Train and Test Sets
LR_train_pred = LR_model.predict(X_train_scaled)
#LR_val_pred=LR_model.predict(X_val_scaled)
LR_test_pred = LR_model.predict(X_test_scaled)

# Evaluation Metrics
train_mse = mean_squared_error(y_train, LR_train_pred)
train_r2 = r2_score(y_train, LR_train_pred)
train_rmse = np.sqrt(train_mse)

#val_r2 = r2_score(y_val, LR_val_pred)

# Print Evaluation Metrics for Training data set
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - LR_train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(LR_train_pred - mean_observed_train)) ** 2))

print('Training Index of Agreement:', round(train_index_of_agreement,2))
print('Training MSE:', round(train_mse, 2))
print('Training R^2:', round(train_r2, 2))
print('Training RMSE:',round(train_rmse, 2))

# Print Evaluation Metrics for Validation data set
#print('\nValidation R2 R^2:', val_r2)  #this parameter will be used to get the best paraper values

#LINEAR REGRESSION
# Print Evaluation Metrics for Test data set
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - LR_test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(LR_test_pred - mean_observed_test)) ** 2))
test_mse = mean_squared_error(y_test, LR_test_pred)
test_r2 = r2_score(y_test, LR_test_pred)
test_rmse = np.sqrt(test_mse)

print('\nTesting Index of Agreement:', round(test_index_of_agreement,2))
print('Testing MSE:', round(test_mse,2))
print('Testing R^2:', round(test_r2,2))
print('Test RMSE:', round(test_rmse,2))

"""MULTIVARIATE POLYNOMIAL REGRESSION"""

#polynomial Regression
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = y_train   # no Scaling
y_val_scaled = y_val       # no scaling
y_test_scaled = y_test     # no scaling

# Importing Polynomial Regression Model
grid_degree = [1,2, 3, 4,5,6,7]
alpha = [1,1e-1,1e-2]
train_r2 = 0
best_r2_val = -float('inf')
best_degree = None
best_alpha = None

for d in grid_degree:
    for a in alpha:
        pr_model = make_pipeline(PolynomialFeatures(degree=d), Ridge(alpha=a))
        pr_model.fit(X_train_scaled, y_train_scaled)

        # Predictions on Train and Validation Sets
        pr_train_pred = pr_model.predict(X_train_scaled)
        pr_val_pred = pr_model.predict(X_val_scaled)
        pr_test_pred=pr_model.predict(X_test_scaled)
        # Evaluation Metrics on Validation Set and Testing set
        val_r2 = r2_score(y_val, pr_val_pred)
        test_r2 = r2_score(y_test,pr_test_pred)
        print("Degree:", d, "| Alpha:", a, "| Validation R^2:", round(val_r2,2), "| Testing R^2:", round(test_r2,2))

        # Check if this degree and alpha gives better R^2 on validation set
        if val_r2 > best_r2_val:
            best_r2_val = val_r2
            best_degree = d
            best_alpha = a

# Train the final model using the best degree and alpha
pr_model = make_pipeline(PolynomialFeatures(degree=best_degree), Ridge(alpha=best_alpha))
pr_model.fit(X_train_scaled, y_train_scaled)

# Predictions on Train and Test Sets
pr_train_pred = pr_model.predict(X_train_scaled)
pr_val_pred = pr_model.predict(X_val_scaled)
pr_test_pred = pr_model.predict(X_test_scaled)

# Evaluation Metrics for training set
train_mse = mean_squared_error(y_train, pr_train_pred)
train_r2 = r2_score(y_train, pr_train_pred)
train_rmse = np.sqrt(train_mse)
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - pr_train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(pr_train_pred - mean_observed_train)) ** 2))
# Print Evaluation Metrics on training data
print('Best Degree:', best_degree)
print('Best Alpha:', best_alpha)
print('\nTraining MSE:', round(train_mse,2))
print('Training R^2:', round(train_r2,2))
print('Training RMSE:', round(train_rmse,2))
print('Training Index of Agreement:', round(train_index_of_agreement,2))

#Evaluation metrics on validation data for r2 value
val_r2 = r2_score(y_val, pr_val_pred)
print('\nValidation R^2:', round(val_r2,2))

#Polynomial Regression
#Evaluation on testing data
test_mse = mean_squared_error(y_test, pr_test_pred)
test_r2 = r2_score(y_test, pr_test_pred)
test_rmse = np.sqrt(test_mse)
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - pr_test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(pr_test_pred - mean_observed_test)) ** 2))
print('\nTesting MSE:',round(test_mse,2))
print('Testing R^2:', round(test_r2,2))
print('Test RMSE:', round(test_rmse,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))

"""validation basis"""

#polynomial Regression
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = y_train   # no Scaling
y_val_scaled = y_val       # no scaling
y_test_scaled = y_test     # no scaling
# Train the final model using the best degree and alpha
pr_model = make_pipeline(PolynomialFeatures(degree=6), Ridge(alpha=1))
pr_model.fit(X_train_scaled, y_train_scaled)

# Predictions on Train and Test Sets
pr_train_pred = pr_model.predict(X_train_scaled)
pr_val_pred = pr_model.predict(X_val_scaled)
pr_test_pred = pr_model.predict(X_test_scaled)

# Evaluation Metrics for training set
train_mse = mean_squared_error(y_train, pr_train_pred)
train_r2 = r2_score(y_train, pr_train_pred)
train_rmse = np.sqrt(train_mse)
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - pr_train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(pr_train_pred - mean_observed_train)) ** 2))
# Print Evaluation Metrics on training data
# print('Best Degree:', best_degree)
# print('Best Alpha:', best_alpha)
print('\nTraining MSE:', round(train_mse,2))
print('Training R^2:', round(train_r2,2))
print('Training RMSE:', round(train_rmse,2))
print('Training Index of Agreement:', round(train_index_of_agreement,2))

#Evaluation metrics on validation data for r2 value
val_r2 = r2_score(y_val, pr_val_pred)
print('\nValidation R^2:', round(val_r2,2))

#Polynomial Regression
#Evaluation on testing data
test_mse = mean_squared_error(y_test, pr_test_pred)
test_r2 = r2_score(y_test, pr_test_pred)
test_rmse = np.sqrt(test_mse)
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - pr_test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(pr_test_pred - mean_observed_test)) ** 2))
print('\nTesting MSE:',round(test_mse,2))
print('Testing R^2:', round(test_r2,2))
print('Test RMSE:', round(test_rmse,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))

"""RECURRENT NEURAL NETWORK"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Assume X_train, X_val, X_test, y_train, y_val, y_test are already defined

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()

# Reshape data for Simple RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_iterations = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8, 16, 32]
h_values = [50, 75, 100, 125, 150, 175, 200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_iterations):
            # Model building and training with Simple RNN
            Rnn_model = Sequential()
            Rnn_model.add(SimpleRNN(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            Rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            Rnn_model.add(Dense(1))
            Rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')
            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            # Fit the model
            history = Rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled),callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test dataset
            rnn_train_pred = Rnn_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(rnn_train_pred.reshape(-1, 1)).ravel()
            rnn_val_pred = Rnn_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(rnn_val_pred.reshape(-1, 1)).ravel()
            rnn_test_pred = Rnn_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(rnn_test_pred.reshape(-1, 1)).ravel()

            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on testing set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)

        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average testing R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2, 2), "| Average Test R^2:", round(avg_test_r2, 2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

# Initialize list to store loss history for each epoch across all iterations
# loss_per_epoch_list = []

# for iteration in range(num_iterations):
#     # Train the final model using the best h value
#     Rnn_model = Sequential()
#     Rnn_model.add(SimpleRNN(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
#     Rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
#     Rnn_model.add(Dense(1))
#     Rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

#     # Early stopping to prevent overfitting
#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
#     # Fit the model
#     history = Rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled),callbacks=[early_stopping], verbose=0)

#     # Append loss history for the current iteration
#     loss_per_epoch_list.append(history.history['loss'])

#     # Plot the loss history for the current iteration
#     plt.plot(range(len(history.history['loss'])), history.history['loss'], marker='o', label=f'Iteration {iteration+1}')
#     plt.xlabel('Epochs')
#     plt.ylabel('Loss')
#     plt.title(f'Training Loss per Epoch for Iteration {iteration+1}')
#     plt.legend()
#     plt.show()

#     # Predictions on train and test
#     rnn_train_pred = Rnn_model.predict(X_train_reshaped).ravel()
#     rnn_test_pred = Rnn_model.predict(X_test_reshaped).ravel()

#     # Inverse transformation
#     train_pred = scaler_y.inverse_transform(rnn_train_pred.reshape(-1, 1)).ravel()
#     test_pred = scaler_y.inverse_transform(rnn_test_pred.reshape(-1, 1)).ravel()

#     # Evaluation metrics
#     train_mse = mean_squared_error(y_train, train_pred)
#     train_r2 = r2_score(y_train, train_pred)
#     train_rmse = np.sqrt(train_mse)
#     test_mse = mean_squared_error(y_test, test_pred)
#     test_r2 = r2_score(y_test, test_pred)
#     test_rmse = np.sqrt(test_mse)

#     # Calculate Index of Agreement for training set
#     mean_observed_train = np.mean(y_train)
#     train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

#     # Calculate Index of Agreement for testing set
#     mean_observed_test = np.mean(y_test)
#     test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#     # Append results to lists
#     train_index_of_agreement_list.append(train_index_of_agreement)
#     test_index_of_agreement_list.append(test_index_of_agreement)
#     train_mse_list.append(train_mse)
#     train_r2_list.append(train_r2)
#     train_rmse_list.append(train_rmse)
#     test_mse_list.append(test_mse)
#     test_r2_list.append(test_r2)
#     test_rmse_list.append(test_rmse)

#Values for simple RNN
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

val_r2 = r2_score(y_val, val_pred)
print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""result for best h and batch size based on validation r2(testing r2)

---



32 batch 100 nodes
"""

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for Simple RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_iterations = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_iterations):
    # Train the final model using the best h value
    Rnn_model = Sequential()
    Rnn_model.add(SimpleRNN(100, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    Rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    Rnn_model.add(Dense(1))
    Rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    # Fit the model
    Rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled),callbacks=[early_stopping], verbose=0)


    # Predictions on train and test
    rnn_train_pred = Rnn_model.predict(X_train_reshaped).ravel()
    rnn_test_pred = Rnn_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(rnn_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(rnn_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for simple RNN
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

# val_r2 = r2_score(y_val, val_pred)
# print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""LSTM+RNN"""

#LONG SHORT TERM MEMORY(LSTM)+RNN
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import LSTM, SimpleRNN, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for LSTM+RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values = [50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store validation R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_runs):
            # Model building and training with LSTM+RNN
            lstm_rnn_model = Sequential()
            lstm_rnn_model.add(LSTM(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
            lstm_rnn_model.add(Dropout(0.1))  # Adding dropout for regularization
            lstm_rnn_model.add(SimpleRNN(50))
            lstm_rnn_model.add(Dropout(0.1))  # Adding dropout for regularization
            lstm_rnn_model.add(Dense(1))
            lstm_rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            lstm_rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test set
            lstm_rnn_train_pred = lstm_rnn_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(lstm_rnn_train_pred.reshape(-1, 1)).ravel()
            lstm_rnn_val_pred = lstm_rnn_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(lstm_rnn_val_pred.reshape(-1, 1)).ravel()
            lstm_rnn_test_pred = lstm_rnn_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(lstm_rnn_test_pred.reshape(-1, 1)).ravel()
            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on test set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)
        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average validation R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)
        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2,2), "| Average Test R^2:", round(avg_test_r2,2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

# Initialize list to store loss history for each epoch across all iterations
loss_per_epoch_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    lstm_rnn_model = Sequential()
    lstm_rnn_model.add(LSTM(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(SimpleRNN(50))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(Dense(1))
    lstm_rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    history = lstm_rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Append loss history for the current iteration
    loss_per_epoch_list.append(history.history['loss'])

    # Plot the loss history for the current iteration
    plt.plot(range(len(history.history['loss'])), history.history['loss'], marker='o', label=f'Iteration {iteration+1}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Training Loss per Epoch for Iteration {iteration+1}')
    plt.legend()
    plt.show()

    # Predictions on train and test
    lstm_rnn_train_pred = lstm_rnn_model.predict(X_train_reshaped).ravel()
    lstm_rnn_test_pred = lstm_rnn_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(lstm_rnn_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(lstm_rnn_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

"""test for best h and batch
32 batch 100 nodes taken from result doc




"""

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for LSTM+RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Number of iterations
num_runs = 10
# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []
for iteration in range(num_runs):
    # Train the final model using the best h value
    lstm_rnn_model = Sequential()
    lstm_rnn_model.add(LSTM(100, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(SimpleRNN(50))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(Dense(1))
    lstm_rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    lstm_rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # # Append loss history for the current iteration
    # loss_per_epoch_list.append(history.history['loss'])

    # # Plot the loss history for the current iteration
    # plt.plot(range(len(history.history['loss'])), history.history['loss'], marker='o', label=f'Iteration {iteration+1}')
    # plt.xlabel('Epochs')
    # plt.ylabel('Loss')
    # plt.title(f'Training Loss per Epoch for Iteration {iteration+1}')
    # plt.legend()
    # plt.show()

    # Predictions on train and test
    lstm_rnn_train_pred = lstm_rnn_model.predict(X_train_reshaped).ravel()
    lstm_rnn_test_pred = lstm_rnn_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(lstm_rnn_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(lstm_rnn_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for rnn+lstm
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""result  for best h and batch based on **testing r2**

32 batch 100 nodes taken from result doc
"""

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for LSTM+RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Number of iterations
num_runs = 10
# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []
for iteration in range(num_runs):
    # Train the final model using the best h value
    lstm_rnn_model = Sequential()
    lstm_rnn_model.add(LSTM(125, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(SimpleRNN(50))
    lstm_rnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_rnn_model.add(Dense(1))
    lstm_rnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    lstm_rnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    lstm_rnn_train_pred = lstm_rnn_model.predict(X_train_reshaped).ravel()
    lstm_rnn_test_pred = lstm_rnn_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(lstm_rnn_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(lstm_rnn_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for rnn+lstm
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""MLPNN"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler

# Assuming X_train, X_val, X_test, y_train, y_val, and y_test are already defined
# Normalize features using only training data
scaler_X = MinMaxScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)  # Scaling validation set
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = y_train   # no Scaling
y_val_scaled = y_val       # no scaling
y_test_scaled = y_test     # no scaling

# List of hidden layer configurations to try
hidden_layer_configurations = [(20,), (40,), (60,), (80,), (100,),
                                (120,), (140,), (160,), (180,),
                                (200,)]

# Initialize lists to store results
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Initialize variables to store best configuration and its corresponding performance
best_config = None
best_val_r2 = -float('inf')

# Fixed optimizer and learning rate
optimizer = 'adam'
learning_rate = 0.1

# Iterate over each hidden layer configuration
for h in hidden_layer_configurations:
    # Importing the Model multilayer perceptron neural network
    mlp_model = MLPRegressor(random_state=42, hidden_layer_sizes=h, max_iter=2000,
                             solver=optimizer, learning_rate_init=learning_rate)
    mlp_model.fit(X_train_scaled, y_train_scaled)

    # Predictions on training data and validation data
    mlp_train_pred = mlp_model.predict(X_train_scaled)
    mlp_val_pred = mlp_model.predict(X_val_scaled)
    mlp_test_pred = mlp_model.predict(X_test_scaled)

    # Evaluation metrics on validation set
    val_r2 = r2_score(y_val, mlp_val_pred)
    test_r2 = r2_score(y_test, mlp_test_pred)

    print("Nodes:", h, "| Optimizer:", optimizer, "| Learning Rate:", learning_rate,
          "| Validation R^2:", round(val_r2, 2), "| Test R^2:", round(test_r2, 2))

    # Check if this configuration performs better than the current best
    if val_r2 > best_val_r2:
        best_val_r2 = val_r2
        best_config = h

# Retrain the model using the best configuration
for i in range(20):
    mlp_model = MLPRegressor(random_state=42, hidden_layer_sizes=best_config, max_iter=2000,
                             solver=optimizer, learning_rate_init=learning_rate)
    mlp_model.fit(X_train_scaled, y_train_scaled)

    # Loss curve
    plt.plot(mlp_model.loss_curve_)
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.show()

    # Predictions on training and testing data
    mlp_train_pred = mlp_model.predict(X_train_scaled)
    mlp_test_pred = mlp_model.predict(X_test_scaled)

    # Finding the METRICS of the MLPNN model on testing set
    train_mse = mean_squared_error(y_train, mlp_train_pred)
    train_r2 = r2_score(y_train, mlp_train_pred)
    train_rmse = np.sqrt(train_mse)

    test_mse = mean_squared_error(y_test, mlp_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, mlp_test_pred)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - mlp_train_pred) ** 2) /
                                   np.sum((np.abs(y_train - mean_observed_train) +
                                           np.abs(mlp_train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - mlp_test_pred) ** 2) /
                                  np.sum((np.abs(y_test - mean_observed_test) +
                                          np.abs(mlp_test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

# Print the best configuration
print(f"Best Configuration: Nodes={best_config}, Optimizer={optimizer}, Learning Rate={learning_rate}")

#MLPNN
#MLPNN
# Calculate  result ON TRAINING DATa
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))
print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))

print('Best hidden layer configuration:', best_config)

val_r2 = r2_score(y_val, mlp_val_pred)
print('\nvalidation on r2:',round(val_r2,2))

avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
print('\nAverage Testing Index of Agreement:', round(avg_test_index_of_agreement,2))
print('Average Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Testing RMSE:', round(avg_test_rmse,2))

"""testing 120 nodes"""

# Retrain the model using the best configuration
for i in range(20):
    mlp_model = MLPRegressor(random_state=42, hidden_layer_sizes=120, max_iter=2000,
                             solver='adam', learning_rate_init=0.1)
    mlp_model.fit(X_train_scaled, y_train_scaled)

    # # Loss curve
    # plt.plot(mlp_model.loss_curve_)
    # plt.title('Loss Curve')
    # plt.xlabel('Epochs')
    # plt.ylabel('Loss')
    # plt.show()

    # Predictions on training and testing data
    mlp_train_pred = mlp_model.predict(X_train_scaled)
    mlp_test_pred = mlp_model.predict(X_test_scaled)

    # Finding the METRICS of the MLPNN model on testing set
    train_mse = mean_squared_error(y_train, mlp_train_pred)
    train_r2 = r2_score(y_train, mlp_train_pred)
    train_rmse = np.sqrt(train_mse)

    test_mse = mean_squared_error(y_test, mlp_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, mlp_test_pred)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - mlp_train_pred) ** 2) /
                                   np.sum((np.abs(y_train - mean_observed_train) +
                                           np.abs(mlp_train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - mlp_test_pred) ** 2) /
                                  np.sum((np.abs(y_test - mean_observed_test) +
                                          np.abs(mlp_test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#MLPNN
#MLPNN
# Calculate  result ON TRAINING DATa
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))
print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))

# print('Best hidden layer configuration:', best_config)

# val_r2 = r2_score(y_val, mlp_val_pred)
# print('\nvalidation on r2:',round(val_r2,2))

avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
print('\nAverage Testing Index of Agreement:', round(avg_test_index_of_agreement,2))
print('Average Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Testing RMSE:', round(avg_test_rmse,2))

"""GATED RECURRENT NETWORK(GRU)"""

#GRU
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values = [50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_runs):
            # Model building and training with GRU
            gru_model = Sequential()
            gru_model.add(GRU(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            gru_model.add(Dense(1))
            gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on testing set and validation set
            gru_train_pred = gru_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(gru_train_pred.reshape(-1, 1)).ravel()
            gru_val_pred = gru_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(gru_val_pred.reshape(-1, 1)).ravel()
            gru_test_pred = gru_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(gru_test_pred.reshape(-1, 1)).ravel()
            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on test set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)
        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average test R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2,2),  "| Average Test R^2:", round(avg_test_r2,2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)
loss_per_epoch_list=[]
for iteration in range(num_runs):
    # Train the final model using the best h value
    gru_model = Sequential()
    gru_model.add(GRU(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    gru_model.add(Dense(1))
    gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    history = gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Append loss history for the current iteration
    loss_per_epoch_list.append(history.history['loss'])

    # Plot the loss history for the current iteration
    plt.plot(range(len(history.history['loss'])), history.history['loss'], marker='o', label=f'Iteration {iteration+1}')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Training Loss per Epoch for Iteration {iteration+1}')
    plt.legend()
    plt.show()
    # Predictions on train and test
    gru_train_pred = gru_model.predict(X_train_reshaped).ravel()
    gru_test_pred = gru_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(gru_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(gru_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#values for GRU MODEL
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

val_r2=r2_score(y_val,val_pred)
print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""test result for best h and best batch for validation r2

32 batch, 200 nodes



"""

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    gru_model = Sequential()
    gru_model.add(GRU(200, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    gru_model.add(Dense(1))
    gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    gru_train_pred = gru_model.predict(X_train_reshaped).ravel()
    gru_test_pred = gru_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(gru_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(gru_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)
#values for GRU MODEL
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

#gru result
print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""test result for best h and best batch for **testing r2**

32 batch, 125 nodes


"""

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    gru_model = Sequential()
    gru_model.add(GRU(125, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    gru_model.add(Dense(1))
    gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    gru_train_pred = gru_model.predict(X_train_reshaped).ravel()
    gru_test_pred = gru_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(gru_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(gru_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)
#values for GRU MODEL
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

#gru result
print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""1D-CNN"""

#1D-CNN
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Initialize variables to store the best filter size and R^2 value
best_filter_size = None
best_val_r2 = -float('inf')

# List of filter sizes to iterate over
filter_sizes = [8, 16, 32, 64, 128]
# Number of iterations for final evaluation
num_iterations = 10

# Lists to store performance metrics for each iteration
train_mse_list = []
train_rmse_list = []
train_r2_list = []
train_ia_list = []
test_mse_list = []
test_rmse_list = []
test_r2_list = []
test_ia_list = []
# Iterate over each filter size
for filter_size in filter_sizes:
    # Model definition
    cnn_model = Sequential([
        Conv1D(filters=filter_size, kernel_size=1, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
        Flatten(),
        Dense(100, activation='relu'),
        Dense(1)
    ])

    cnn_model.compile(optimizer='adam', loss='mse')
    # Fitting the model
    cnn_model.fit(X_train_scaled[..., np.newaxis], y_train_scaled, epochs=100, batch_size=32, verbose=0)

    # predictions on training set and validation set
    val_pred = cnn_model.predict(X_val_scaled[..., np.newaxis])
    val_pred_inv = scaler_y.inverse_transform(val_pred).ravel()

    # predictions on test set
    test_pred = cnn_model.predict(X_test_scaled[..., np.newaxis])
    test_pred_inv = scaler_y.inverse_transform(test_pred).ravel()


    # Calculate R^2 score
    val_r2 = r2_score(y_val, val_pred_inv)
    test_r2= r2_score(y_test, test_pred_inv)
    print("filter :" ,filter_size,"| Validation R^2:", round(val_r2,2),"| Test R^2:", round(test_r2,2))

    # Check if this filter size gives better R^2
    if val_r2 > best_val_r2:
        best_val_r2 = val_r2
        best_filter_size = filter_size

# Perform iterations
for iteration in range(num_iterations):
    # Model definition using the best filter size
    cnn_model = Sequential([
        Conv1D(filters=best_filter_size, kernel_size=1, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
        Flatten(),
        Dense(100, activation='relu'),
        Dense(1)
    ])

    cnn_model.compile(optimizer='adam', loss='mse')

    # Fit the final model
    cnn_model.fit(X_train_scaled[..., np.newaxis], y_train_scaled, epochs=100, batch_size=32, verbose=0)

    # predictions on training and testing set
    train_pred = cnn_model.predict(X_train_scaled[..., np.newaxis])
    train_pred_inv = scaler_y.inverse_transform(train_pred).ravel()
    test_pred = cnn_model.predict(X_test_scaled[..., np.newaxis])
    test_pred_inv = scaler_y.inverse_transform(test_pred).ravel()

    # Evaluate performance on testing set
    test_mse = mean_squared_error(y_test, test_pred_inv)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, test_pred_inv)

    # Evaluate performance on training set
    train_mse = mean_squared_error(y_train, train_pred_inv)
    train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(y_train, train_pred_inv)
    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_ia = 1 - (np.sum((y_train - train_pred_inv) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred_inv - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_ia = 1 - (np.sum((y_test - test_pred_inv) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred_inv - mean_observed_test)) ** 2))

    # Append metrics to lists
    train_mse_list.append(train_mse)
    train_rmse_list.append(train_rmse)
    train_r2_list.append(train_r2)
    train_ia_list.append(train_ia)
    test_mse_list.append(test_mse)
    test_rmse_list.append(test_rmse)
    test_r2_list.append(test_r2)
    test_ia_list.append(test_ia)

# Calculate average performance metrics
avg_train_mse = np.mean(train_mse_list)
avg_train_rmse = np.mean(train_rmse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_index_of_agreement = np.mean(train_ia_list)
print("Average Training MSE:", round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# Print the best R2 value on validation set
print('\nBest h value based on R^2:',best_filter_size )
val_r2 = r2_score(y_val, val_pred)
print('\nValidation R^2:', round(best_val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_index_of_agreement = np.mean(test_ia_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""testing low node value"""

#1D-CNN
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Number of iterations for final evaluation
num_iterations = 10
# Lists to store performance metrics for each iteration
train_mse_list = []
train_rmse_list = []
train_r2_list = []
train_ia_list = []
test_mse_list = []
test_rmse_list = []
test_r2_list = []
test_ia_list = []
# Perform iterations
for iteration in range(num_iterations):
    # Model definition using the best filter size
    cnn_model = Sequential([
        Conv1D(filters=16, kernel_size=1, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),
        Flatten(),
        Dense(100, activation='relu'),
        Dense(1)
    ])

    cnn_model.compile(optimizer='adam', loss='mse')

    # Fit the final model
    cnn_model.fit(X_train_scaled[..., np.newaxis], y_train_scaled, epochs=100, batch_size=32, verbose=0)

    # predictions on training and testing set
    train_pred = cnn_model.predict(X_train_scaled[..., np.newaxis])
    train_pred_inv = scaler_y.inverse_transform(train_pred).ravel()
    test_pred = cnn_model.predict(X_test_scaled[..., np.newaxis])
    test_pred_inv = scaler_y.inverse_transform(test_pred).ravel()

    # Evaluate performance on testing set
    test_mse = mean_squared_error(y_test, test_pred_inv)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, test_pred_inv)

    # Evaluate performance on training set
    train_mse = mean_squared_error(y_train, train_pred_inv)
    train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(y_train, train_pred_inv)
    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_ia = 1 - (np.sum((y_train - train_pred_inv) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred_inv - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_ia = 1 - (np.sum((y_test - test_pred_inv) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred_inv - mean_observed_test)) ** 2))

    # Append metrics to lists
    train_mse_list.append(train_mse)
    train_rmse_list.append(train_rmse)
    train_r2_list.append(train_r2)
    train_ia_list.append(train_ia)
    test_mse_list.append(test_mse)
    test_rmse_list.append(test_rmse)
    test_r2_list.append(test_r2)
    test_ia_list.append(test_ia)

# Calculate average performance metrics
avg_train_mse = np.mean(train_mse_list)
avg_train_rmse = np.mean(train_rmse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_index_of_agreement = np.mean(train_ia_list)
print("Average Training MSE:", round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# Print the best R2 value on validation set
# print('\nBest h value based on R^2:',best_filter_size )
# val_r2 = r2_score(y_val, val_pred)
# print('\nValidation R^2:', round(best_val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_index_of_agreement = np.mean(test_ia_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""GRU+LSTM"""

#GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values = [50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values and test R^2 values for each iteration
        val_r2_values = []
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_runs):
            # Model building and training with GRU and LSTM
            model = Sequential()
            model.add(GRU(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
            model.add(LSTM(100))
            model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            model.add(Dense(1))
            model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set
            train_pred = model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
            val_pred = model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(val_pred.reshape(-1, 1)).ravel()
            test_pred= model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()
            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on test set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)
        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average test R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)
        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2,2),  "| Average Test R^2:", round(avg_test_r2,2))


        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)
# loss_per_epoch_list=[]
# for iteration in range(num_runs):
#     # Train the final model using the best h value
#     model = Sequential()
#     model.add(GRU(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
#     model.add(LSTM(best_h))
#     model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
#     model.add(Dense(1))
#     model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

#     # Early stopping to prevent overfitting
#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

#     # Fit the model on entire training data
#     model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=0)
#     #loss
#     loss_per_epoch_list.append(model.history.history['loss'])
#     # Predictions on train and test
#     train_pred = model.predict(X_train_reshaped).ravel()
#     test_pred = model.predict(X_test_reshaped).ravel()

#     # Inverse transformation
#     train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
#     test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

#     # Evaluation metrics
#     train_mse = mean_squared_error(y_train, train_pred)
#     train_r2 = r2_score(y_train, train_pred)
#     train_rmse = np.sqrt(train_mse)
#     test_mse = mean_squared_error(y_test, test_pred)
#     test_r2 = r2_score(y_test, test_pred)
#     test_rmse = np.sqrt(test_mse)

#     # Calculate Index of Agreement for training set
#     mean_observed_train = np.mean(y_train)
#     train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

#     # Calculate Index of Agreement for testing set
#     mean_observed_test = np.mean(y_test)
#     test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#     # Append results to lists
#     train_index_of_agreement_list.append(train_index_of_agreement)
#     test_index_of_agreement_list.append(test_index_of_agreement)
#     train_mse_list.append(train_mse)
#     train_r2_list.append(train_r2)
#     train_rmse_list.append(train_rmse)
#     test_mse_list.append(test_mse)
#     test_r2_list.append(test_r2)
#     test_rmse_list.append(test_rmse)

# Plot the loss history for each iteration
for i, loss_per_epoch in enumerate(loss_per_epoch_list):
    plt.plot(range(len(loss_per_epoch)), loss_per_epoch, label=f'Iteration {i+1}')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss per Epoch for Each Iteration')
plt.legend()
plt.show()

#Values for  gru+lstm
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', avg_train_mse)
print('Average Training R^2:', avg_train_r2)
print('Average Training RMSE:', avg_train_rmse)
print('Average Training Index of Agreement:', avg_train_index_of_agreement)

val_r2=r2_score(y_val,val_pred)
print('\n Validation value of r2 score:',val_r2)

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', avg_test_mse)
print('Average Testing R^2:', avg_test_r2)
print('Average Test RMSE:', avg_test_rmse)
print('Average Testing Index of Agreement:', avg_test_index_of_agreement)

"""final model of gru+lstm(100nodes) based on val r2

32 batch size, 100 nodes
"""

#GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []
for iteration in range(num_runs):
    # Train the final model using the best h value
    model = Sequential()
    model.add(GRU(100, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
    model.add(LSTM(100))
    model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32,validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    train_pred = model.predict(X_train_reshaped).ravel()
    test_pred = model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

# val_r2 = r2_score(y_val, val_pred)
# print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""final model of gru+lstm(100nodes) based on testing r2

32 batch size, 175 nodes
"""

#GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []
for iteration in range(num_runs):
    # Train the final model using the best h value
    model = Sequential()
    model.add(GRU(175, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]), return_sequences=True))
    model.add(LSTM(100))
    model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32,validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    train_pred = model.predict(X_train_reshaped).ravel()
    test_pred = model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

# val_r2 = r2_score(y_val, val_pred)
# print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""RBFN"""

#RBFNN
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
X_val_scaled = scaler_X.transform(X_val)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Number of RBF centers
n_centers = min(100, len(X_train_scaled))

# Use KMeans to get the centers
kmeans = KMeans(n_clusters=n_centers, random_state=42).fit(X_train_scaled)
centers = kmeans.cluster_centers_

# Calculate the widths
distances = euclidean_distances(centers, [centers[0]])
median_distance = np.median(distances)
sigma = median_distance / np.sqrt(2 * np.log(2))

# Initialize variables to store best alpha and corresponding performance
best_alpha = None
best_val_r2 = -float('inf')

# Iterate over each alpha value
for alpha in [1, 0.1, 0.01, 0.001]:
    # Calculate RBF features
    X_train_rbf = rbf_kernel(X_train_scaled, centers, gamma=1.0 / sigma)
    X_val_rbf = rbf_kernel(X_val_scaled, centers, gamma=1.0 / sigma)
    X_test_rbf = rbf_kernel(X_test_scaled, centers, gamma=1.0 / sigma)
    # Model the Radial Basis Function Network
    rbf_pipeline = make_pipeline(Ridge(alpha=alpha, fit_intercept=False))
    rbf_pipeline.fit(X_train_rbf, y_train_scaled)

    # Prediction for validation and testing datasets
    rbfn_val_pred = rbf_pipeline.predict(X_val_rbf)
    rbfn_test_pred = rbf_pipeline.predict(X_test_rbf)

    # Inverse transform the predictions to get them back to the original scale
    val_pred = scaler_y.inverse_transform(rbfn_val_pred.reshape(-1, 1)).flatten()
    test_pred = scaler_y.inverse_transform(rbfn_test_pred.reshape(-1, 1)).flatten()

    # Finding the r^2 of the RBFN model
    val_r2 = r2_score(y_val, val_pred)
    test_r2 = r2_score(y_test, test_pred)

    print("Alpha Values:", alpha, "| Validation R^2:", round(val_r2,2),  "| Test R^2:", round(test_r2,2))

    # Check if this alpha performs better than the current best
    if val_r2 > best_val_r2:
        best_val_r2 = val_r2
        best_alpha = alpha

# Use the best alpha value on the testing set
X_test_rbf = rbf_kernel(X_test_scaled, centers, gamma=1.0 / sigma)
rbf_pipeline = make_pipeline(Ridge(alpha=best_alpha, fit_intercept=False))
rbf_pipeline.fit(X_train_rbf, y_train_scaled)
rbfn_test_pred = rbf_pipeline.predict(X_test_rbf)
# Use the best alpha value on the training set
rbf_pipeline_train = make_pipeline(Ridge(alpha=best_alpha, fit_intercept=False))
rbf_pipeline_train.fit(X_train_rbf, y_train_scaled)
rbfn_train_pred = rbf_pipeline_train.predict(X_train_rbf)

# Inverse transform the predictions to get them back to the original scale
train_pred = scaler_y.inverse_transform(rbfn_train_pred.reshape(-1, 1)).flatten()
test_pred = scaler_y.inverse_transform(rbfn_test_pred.reshape(-1, 1)).flatten()

# Finding the mse of the RBFN model on testing set
train_mse = mean_squared_error(y_train, train_pred)
train_r2 = r2_score(y_train, train_pred)
train_rmse = np.sqrt(train_mse)

test_mse = mean_squared_error(y_test, test_pred)
test_rmse = np.sqrt(test_mse)
test_r2 = r2_score(y_test, test_pred)
# Calculate Index of Agreement for training set
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

# Calculate Index of Agreement for testing set
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#RBFNN
print('Training MSE:', round(train_mse,2))
print('Training RMSE:', round(train_rmse,2))
print('Training R^2:', round(train_r2,2))
print('Training Index of Agreement:', round(train_index_of_agreement,2))

print('\nBest alpha value:', best_alpha)

val_r2 = r2_score(y_val, val_pred)
print('Validation r2 value:',round(val_r2,2))

print('\nTesting MSE:', round(test_mse,2))
print('Testing RMSE:', round(test_rmse,2))
print('Testing R^2:', round(test_r2,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))

"""Based on testing r2

aplha=1;
"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances, rbf_kernel
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
X_val_scaled = scaler_X.transform(X_val)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()

# Number of RBF centers
n_centers = min(100, len(X_train_scaled))

# Use KMeans to get the centers
kmeans = KMeans(n_clusters=n_centers, random_state=42).fit(X_train_scaled)
centers = kmeans.cluster_centers_

# Calculate the widths
distances = euclidean_distances(centers, centers)
median_distance = np.median(distances)
sigma = median_distance / np.sqrt(2 * np.log(2))

# Transform the training data using RBF kernel
X_train_rbf = rbf_kernel(X_train_scaled, centers, gamma=1.0 / sigma)
X_test_rbf = rbf_kernel(X_test_scaled, centers, gamma=1.0 / sigma)
X_val_rbf = rbf_kernel(X_val_scaled, centers, gamma=1.0 / sigma)

# Fit the model on the training set
rbf_pipeline = make_pipeline(Ridge(alpha=1, fit_intercept=False))
rbf_pipeline.fit(X_train_rbf, y_train_scaled)

# Predict on the training and testing set
rbfn_train_pred = rbf_pipeline.predict(X_train_rbf)
rbfn_test_pred = rbf_pipeline.predict(X_test_rbf)

# Inverse transform the predictions to get them back to the original scale
train_pred = scaler_y.inverse_transform(rbfn_train_pred.reshape(-1, 1)).flatten()
test_pred = scaler_y.inverse_transform(rbfn_test_pred.reshape(-1, 1)).flatten()

# Finding the mse of the RBFN model on training and testing sets
train_mse = mean_squared_error(y_train, train_pred)
train_rmse = np.sqrt(train_mse)
train_r2 = r2_score(y_train, train_pred)

test_mse = mean_squared_error(y_test, test_pred)
test_rmse = np.sqrt(test_mse)
test_r2 = r2_score(y_test, test_pred)

# Calculate Index of Agreement for training set
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) /
                                np.sum((np.abs(y_train - mean_observed_train) +
                                        np.abs(train_pred - mean_observed_train)) ** 2))

# Calculate Index of Agreement for testing set
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) /
                               np.sum((np.abs(y_test - mean_observed_test) +
                                       np.abs(test_pred - mean_observed_test)) ** 2))

#RBFNN
print('Training MSE:', round(train_mse,2))
print('Training RMSE:', round(train_rmse,2))
print('Training R^2:', round(train_r2,2))
print('Training Index of Agreement:', round(train_index_of_agreement,2))

# print('\nBest alpha value:', best_alpha)

# val_r2 = r2_score(y_val, val_pred)
# print('Validation r2 value:',round(val_r2,2))

print('\nTesting MSE:', round(test_mse,2))
print('Testing RMSE:', round(test_rmse,2))
print('Testing R^2:', round(test_r2,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))

"""BI-DIRECTIONAL GRU"""

#Bi-directional gru
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for Bidirectional GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values = [50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_runs):
            # Model building and training with Bidirectional GRU
            bidirectional_gru_model = Sequential()
            bidirectional_gru_model.add(Bidirectional(GRU(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))))
            bidirectional_gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            bidirectional_gru_model.add(Dense(1))
            bidirectional_gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            bidirectional_gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test set
            bidirectional_gru_train_pred = bidirectional_gru_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(bidirectional_gru_train_pred.reshape(-1, 1)).ravel()
            bidirectional_gru_val_pred = bidirectional_gru_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(bidirectional_gru_val_pred.reshape(-1, 1)).ravel()
            bidirectional_gru_test_pred = bidirectional_gru_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(bidirectional_gru_test_pred.reshape(-1, 1)).ravel()
            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on test set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)
        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average test R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 and test R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2,2),  "| Average Test R^2:", round(avg_test_r2,2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

# for iteration in range(num_runs):
#     # Train the final model using the best h value
#     bidirectional_gru_model = Sequential()
#     bidirectional_gru_model.add(Bidirectional(GRU(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))))
#     bidirectional_gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
#     bidirectional_gru_model.add(Dense(1))
#     bidirectional_gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

#     # Early stopping to prevent overfitting
#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

#     # Fit the model on entire training data
#     bidirectional_gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

#     # Predictions on train and test
#     bidirectional_gru_train_pred = bidirectional_gru_model.predict(X_train_reshaped).ravel()
#     bidirectional_gru_test_pred = bidirectional_gru_model.predict(X_test_reshaped).ravel()

#     # Inverse transformation
#     train_pred = scaler_y.inverse_transform(bidirectional_gru_train_pred.reshape(-1, 1)).ravel()
#     test_pred = scaler_y.inverse_transform(bidirectional_gru_test_pred.reshape(-1, 1)).ravel()

#     # Evaluation metrics
#     train_mse = mean_squared_error(y_train, train_pred)
#     train_r2 = r2_score(y_train, train_pred)
#     train_rmse = np.sqrt(train_mse)
#     test_mse = mean_squared_error(y_test, test_pred)
#     test_r2 = r2_score(y_test, test_pred)
#     test_rmse = np.sqrt(test_mse)

#     # Calculate Index of Agreement for training set
#     mean_observed_train = np.mean(y_train)
#     train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

#     # Calculate Index of Agreement for testing set
#     mean_observed_test = np.mean(y_test)
#     test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#     # Append results to lists
#     train_index_of_agreement_list.append(train_index_of_agreement)
#     test_index_of_agreement_list.append(test_index_of_agreement)
#     train_mse_list.append(train_mse)
#     train_r2_list.append(train_r2)
#     train_rmse_list.append(train_rmse)
#     test_mse_list.append(test_mse)
#     test_r2_list.append(test_r2)
#     test_rmse_list.append(test_rmse)

#Values for Bi-directional gru
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', avg_train_mse)
print('Average Training R^2:', avg_train_r2)
print('Average Training RMSE:', avg_train_rmse)
print('Average Training Index of Agreement:', avg_train_index_of_agreement)

val_r2=r2_score(y_val,val_pred)
print('\n Validation value of r2 score:',val_r2)

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', avg_test_mse)
print('Average Testing R^2:', avg_test_r2)
print('Average Test RMSE:', avg_test_rmse)
print('Average Testing Index of Agreement:', avg_test_index_of_agreement)

"""result for best h and best batch based on validation r2
32 batch 50 nodes
"""

#Bi-directional gru
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for Bidirectional GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    bidirectional_gru_model = Sequential()
    bidirectional_gru_model.add(Bidirectional(GRU(50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))))
    bidirectional_gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    bidirectional_gru_model.add(Dense(1))
    bidirectional_gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    bidirectional_gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    bidirectional_gru_train_pred = bidirectional_gru_model.predict(X_train_reshaped).ravel()
    bidirectional_gru_test_pred = bidirectional_gru_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(bidirectional_gru_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(bidirectional_gru_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for Bi-directional gru
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""result for best h and best batch based on **testing r2**

32 batch 100 nodes
"""

#Bi-directional gru
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for Bidirectional GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    bidirectional_gru_model = Sequential()
    bidirectional_gru_model.add(Bidirectional(GRU(100, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2]))))
    bidirectional_gru_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    bidirectional_gru_model.add(Dense(1))
    bidirectional_gru_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    bidirectional_gru_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    bidirectional_gru_train_pred = bidirectional_gru_model.predict(X_train_reshaped).ravel()
    bidirectional_gru_test_pred = bidirectional_gru_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(bidirectional_gru_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(bidirectional_gru_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for Bi-directional gru
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""BI-DIRECTIONAL GRU+LSTM"""

#Bi-directional GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values = [50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_runs):
            # Model building and training with bidirectional GRU and LSTM
            bi_gru_lstm_model = Sequential()
            bi_gru_lstm_model.add(Bidirectional(GRU(h, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            bi_gru_lstm_model.add(Bidirectional(LSTM(50)))
            bi_gru_lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            bi_gru_lstm_model.add(Dense(1))
            bi_gru_lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            bi_gru_lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and testing set
            train_pred = bi_gru_lstm_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
            val_pred = bi_gru_lstm_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(val_pred.reshape(-1, 1)).ravel()
            test_pred = bi_gru_lstm_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)

            # Evaluation metrics on test set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)

        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)

        # Calculate average test R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 and test R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2,2),  "| Average Test R^2:", round(avg_test_r2,2))
        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

# for iteration in range(num_iterations):
#     # Train the final model using the best h value
#     bi_gru_lstm_model = Sequential()
#     bi_gru_lstm_model.add(Bidirectional(GRU(best_h, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
#     bi_gru_lstm_model.add(Bidirectional(LSTM(best_h)))
#     bi_gru_lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
#     bi_gru_lstm_model.add(Dense(1))
#     bi_gru_lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

#     # Early stopping to prevent overfitting
#     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

#     # Fit the model on entire training data
#     bi_gru_lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=5000, batch_size=best_batch_size, validation_split=0.2, callbacks=[early_stopping], verbose=0)

#     # Predictions on train and test
#     train_pred = bi_gru_lstm_model.predict(X_train_reshaped).ravel()
#     test_pred = bi_gru_lstm_model.predict(X_test_reshaped).ravel()

#     # Inverse transformation
#     train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
#     test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

#     # Evaluation metrics
#     train_mse = mean_squared_error(y_train, train_pred)
#     train_r2 = r2_score(y_train, train_pred)
#     train_rmse = np.sqrt(train_mse)
#     test_mse = mean_squared_error(y_test, test_pred)
#     test_r2 = r2_score(y_test, test_pred)
#     test_rmse = np.sqrt(test_mse)

#     # Calculate Index of Agreement for training set
#     mean_observed_train = np.mean(y_train)
#     train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

#     # Calculate Index of Agreement for testing set
#     mean_observed_test = np.mean(y_test)
#     test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#     # Append results to lists
#     train_index_of_agreement_list.append(train_index_of_agreement)
#     test_index_of_agreement_list.append(test_index_of_agreement)
#     train_mse_list.append(train_mse)
#     train_r2_list.append(train_r2)
#     train_rmse_list.append(train_rmse)
#     test_mse_list.append(test_mse)
#     test_r2_list.append(test_r2)
#     test_rmse_list.append(test_rmse)

#Values for Bi-directional gru+lstm
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', avg_train_mse)
print('Average Training R^2:', avg_train_r2)
print('Average Training RMSE:', avg_train_rmse)
print('Average Training Index of Agreement:', avg_train_index_of_agreement)

val_r2=r2_score(y_val,val_pred)
print('\n Validation value of r2 score:',val_r2)

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', avg_test_mse)
print('Average Testing R^2:', avg_test_r2)
print('Average Test RMSE:', avg_test_rmse)
print('Average Testing Index of Agreement:', avg_test_index_of_agreement)

"""Bi_Gru+Lstm(50) final result based on val r2

8 batch size, 175 nodes
"""

#Bi-directional GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    bi_gru_lstm_model = Sequential()
    bi_gru_lstm_model.add(Bidirectional(GRU(175, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    bi_gru_lstm_model.add(Bidirectional(LSTM(50)))
    bi_gru_lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    bi_gru_lstm_model.add(Dense(1))
    bi_gru_lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    bi_gru_lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=8, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    train_pred = bi_gru_lstm_model.predict(X_train_reshaped).ravel()
    test_pred = bi_gru_lstm_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for Bi-directional gru
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""Based on testing r2(bi_gru+lstm(50nodes))

32 batch size 150 nodes
"""

#Bi-directional GRU+LSTM
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()
# Reshape data for GRU
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Number of iterations
num_runs = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

for iteration in range(num_runs):
    # Train the final model using the best h value
    bi_gru_lstm_model = Sequential()
    bi_gru_lstm_model.add(Bidirectional(GRU(150, return_sequences=True), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    bi_gru_lstm_model.add(Bidirectional(LSTM(50)))
    bi_gru_lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    bi_gru_lstm_model.add(Dense(1))
    bi_gru_lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    bi_gru_lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=32, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    train_pred = bi_gru_lstm_model.predict(X_train_reshaped).ravel()
    test_pred = bi_gru_lstm_model.predict(X_test_reshaped).ravel()

    # Inverse transformation
    train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for Bi-directional gru
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print('Average Training MSE:', round(avg_train_mse,2))
print('Average Training R^2:', round(avg_train_r2,2))
print('Average Training RMSE:', round(avg_train_rmse,2))
print('Average Training Index of Agreement:', round(avg_train_index_of_agreement,2))

# val_r2=r2_score(y_val,val_pred)
# print('\n Validation value of r2 score:',round(val_r2,2))

# Evaluation metrics for testing set
avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)
print('\nAverage Testing MSE:', round(avg_test_mse,2))
print('Average Testing R^2:', round(avg_test_r2,2))
print('Average Test RMSE:', round(avg_test_rmse,2))
print('Average Testing Index of Agreement:', round(avg_test_index_of_agreement,2))

"""#LSTM"""

# Import necessary libraries
from keras.layers import LSTM, Dropout, Dense

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for LSTM
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_iterations = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8,16,32]
h_values=[50,75,100,125,150,175,200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_iterations):
            # Model building and training with LSTM
            lstm_model = Sequential()
            lstm_model.add(LSTM(h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            lstm_model.add(Dense(1))
            lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test dataset
            lstm_train_pred = lstm_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(lstm_train_pred.reshape(-1, 1)).ravel()
            lstm_val_pred = lstm_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(lstm_val_pred.reshape(-1, 1)).ravel()
            lstm_test_pred = lstm_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(lstm_test_pred.reshape(-1, 1)).ravel()

            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on testing set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)

        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average testing R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2, 2), "| Average Test R^2:", round(avg_test_r2, 2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

for iteration in range(num_iterations):
  # Train the final model using the best h value
  lstm_model = Sequential()
  lstm_model.add(LSTM(best_h, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
  lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
  lstm_model.add(Dense(1))
  lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

  # Early stopping to prevent overfitting
  early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

  # Fit the model on entire training data
  lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

  # Predictions on train and test
  lstm_train_pred = lstm_model.predict(X_train_reshaped).ravel()
  lstm_test_pred = lstm_model.predict(X_test_reshaped).ravel()
  # Inverse transformation
  train_pred = scaler_y.inverse_transform(lstm_train_pred.reshape(-1, 1)).ravel()
  test_pred = scaler_y.inverse_transform(lstm_test_pred.reshape(-1, 1)).ravel()

  # Evaluation metrics
  train_mse = mean_squared_error(y_train, train_pred)
  train_r2 = r2_score(y_train, train_pred)
  train_rmse = np.sqrt(train_mse)
  test_mse = mean_squared_error(y_test, test_pred)
  test_r2 = r2_score(y_test, test_pred)
  test_rmse = np.sqrt(test_mse)

  # Calculate Index of Agreement for training set
  mean_observed_train = np.mean(y_train)
  train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

  # Calculate Index of Agreement for testing set
  mean_observed_test = np.mean(y_test)
  test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

  # Append results to lists
  train_index_of_agreement_list.append(train_index_of_agreement)
  test_index_of_agreement_list.append(test_index_of_agreement)
  train_mse_list.append(train_mse)
  train_r2_list.append(train_r2)
  train_rmse_list.append(train_rmse)
  test_mse_list.append(test_mse)
  test_r2_list.append(test_r2)
  test_rmse_list.append(test_rmse)

#Values for lstm
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

val_r2 = r2_score(y_val, val_pred)
print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""#AT-LSTM"""

# Import necessary libraries
from keras.layers import LSTM, Dropout, Dense, Input, Concatenate
from keras.layers import Layer
from keras import backend as K
import tensorflow as tf

# Custom attention layer
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)
        self.u = self.add_weight(name='attention_context_vector', shape=(input_shape[-1],), initializer='random_normal', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        # uit has shape (batch_size, time_steps, input_dim)
        uit = K.tanh(K.dot(x, self.W) + self.b)  # shape: (batch_size, time_steps, input_dim)

        # Reshape self.u to be compatible with uit for dot product
        uit_shape = K.shape(uit)
        u_expanded = K.expand_dims(self.u, axis=0)  # shape: (1, input_dim)
        u_expanded = K.tile(u_expanded, [uit_shape[0], 1])  # shape: (batch_size, input_dim)
        u_expanded = K.expand_dims(u_expanded, axis=1)  # shape: (batch_size, 1, input_dim)

        ait = K.sum(uit * u_expanded, axis=-1)  # shape: (batch_size, time_steps)
        ait = K.softmax(ait)  # shape: (batch_size, time_steps)

        ait = K.expand_dims(ait)  # shape: (batch_size, time_steps, 1)
        weighted_input = x * ait  # shape: (batch_size, time_steps, input_dim)
        output = K.sum(weighted_input, axis=1)  # shape: (batch_size, input_dim)
        return output

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for LSTM
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_iterations = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8, 16, 32]
h_values = [50, 75, 100, 125, 150, 175, 200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_iterations):
            # Model building and training with Attention LSTM
            lstm_model = Sequential()
            lstm_model.add(LSTM(h, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            lstm_model.add(Attention())
            lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            lstm_model.add(Dense(1))
            lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test dataset
            lstm_train_pred = lstm_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(lstm_train_pred.reshape(-1, 1)).ravel()
            lstm_val_pred = lstm_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(lstm_val_pred.reshape(-1, 1)).ravel()
            lstm_test_pred = lstm_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(lstm_test_pred.reshape(-1, 1)).ravel()

            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on testing set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)

        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average testing R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2, 2), "| Average Test R^2:", round(avg_test_r2, 2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

for iteration in range(num_iterations):
    # Train the final model using the best h value
    lstm_model = Sequential()
    lstm_model.add(LSTM(best_h, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    lstm_model.add(Attention())
    lstm_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    lstm_model.add(Dense(1))
    lstm_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    history = lstm_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    lstm_train_pred = lstm_model.predict(X_train_reshaped).ravel()
    lstm_test_pred = lstm_model.predict(X_test_reshaped).ravel()
    # Inverse transformation
    train_pred = scaler_y.inverse_transform(lstm_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(lstm_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for AT-LSTM
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

val_r2 = r2_score(y_val, val_pred)
print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""#DRNN"""

#drnn
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Reshape data for RNN
X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))
X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Initialize variables to store best h and average validation R^2 value
best_h = None
best_batch_size = None
best_avg_val_r2 = -float('inf')

# Number of iterations
num_iterations = 10

# Lists to store metrics for each iteration
train_index_of_agreement_list = []
test_index_of_agreement_list = []
train_mse_list = []
train_r2_list = []
train_rmse_list = []
test_mse_list = []
test_r2_list = []
test_rmse_list = []

# Batch sizes to try
batch_sizes = [8, 16, 32]
h_values = [50, 75, 100, 125, 150, 175, 200]

# Iterate through each value of batch size
for batch_size in batch_sizes:
    # Iterate through each value of h
    for h in h_values:
        # Lists to store validation R^2 values for each iteration
        val_r2_values = []
        # Lists to store test R^2 values for each iteration
        test_r2_values = []
        # Perform multiple iterations for each h value and batch size
        for _ in range(num_iterations):
            # Model building and training with DRNN
            drnn_model = Sequential()
            drnn_model.add(SimpleRNN(h, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
            drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            drnn_model.add(SimpleRNN(50, return_sequences=True))
            drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            drnn_model.add(SimpleRNN(50))
            drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
            drnn_model.add(Dense(1))
            drnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

            # Early stopping to prevent overfitting
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

            # Fit the model
            drnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

            # Predictions on training set and validation set and test dataset
            drnn_train_pred = drnn_model.predict(X_train_reshaped).ravel()
            train_pred = scaler_y.inverse_transform(drnn_train_pred.reshape(-1, 1)).ravel()
            drnn_val_pred = drnn_model.predict(X_val_reshaped).ravel()
            val_pred = scaler_y.inverse_transform(drnn_val_pred.reshape(-1, 1)).ravel()
            drnn_test_pred = drnn_model.predict(X_test_reshaped).ravel()
            test_pred = scaler_y.inverse_transform(drnn_test_pred.reshape(-1, 1)).ravel()

            # Evaluation metrics on validation set
            val_r2 = r2_score(y_val, val_pred)
            val_r2_values.append(val_r2)
            # Evaluation metrics on testing set
            test_r2 = r2_score(y_test, test_pred)
            test_r2_values.append(test_r2)

        # Calculate average validation R^2 for this h value and batch size
        avg_val_r2 = np.mean(val_r2_values)
        # Calculate average testing R^2 for this h value and batch size
        avg_test_r2 = np.mean(test_r2_values)

        # Print average validation R^2 for this h value and batch size
        print("Batch size:", batch_size, "| Node:", h, "| Average Validation R^2:", round(avg_val_r2, 2), "| Average Test R^2:", round(avg_test_r2, 2))

        # Check if this h value and batch size combination gives a better average validation R^2
        if avg_val_r2 > best_avg_val_r2:
            best_avg_val_r2 = avg_val_r2
            best_h = h
            best_batch_size = batch_size

# Print the best h value and batch size
print("Best h value:", best_h)
print("Best batch size:", best_batch_size)

loss_per_epoch_list = []
for iteration in range(num_iterations):
    # Train the final model using the best h value
    drnn_model = Sequential()
    drnn_model.add(SimpleRNN(best_h, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))
    drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    drnn_model.add(SimpleRNN(50, return_sequences=True))
    drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    drnn_model.add(SimpleRNN(50))
    drnn_model.add(Dropout(0.1))  # Adding dropout with a rate of 0.1
    drnn_model.add(Dense(1))
    drnn_model.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')

    # Early stopping to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Fit the model on entire training data
    drnn_model.fit(X_train_reshaped, y_train_scaled, epochs=2000, batch_size=best_batch_size, validation_data=(X_val_reshaped, y_val_scaled), callbacks=[early_stopping], verbose=0)

    # Predictions on train and test
    drnn_train_pred = drnn_model.predict(X_train_reshaped).ravel()
    drnn_test_pred = drnn_model.predict(X_test_reshaped).ravel()
    # Inverse transformation
    train_pred = scaler_y.inverse_transform(drnn_train_pred.reshape(-1, 1)).ravel()
    test_pred = scaler_y.inverse_transform(drnn_test_pred.reshape(-1, 1)).ravel()

    # Evaluation metrics
    train_mse = mean_squared_error(y_train, train_pred)
    train_r2 = r2_score(y_train, train_pred)
    train_rmse = np.sqrt(train_mse)
    test_mse = mean_squared_error(y_test, test_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(test_mse)

    # Calculate Index of Agreement for training set
    mean_observed_train = np.mean(y_train)
    train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

    # Calculate Index of Agreement for testing set
    mean_observed_test = np.mean(y_test)
    test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

    # Append results to lists
    train_index_of_agreement_list.append(train_index_of_agreement)
    test_index_of_agreement_list.append(test_index_of_agreement)
    train_mse_list.append(train_mse)
    train_r2_list.append(train_r2)
    train_rmse_list.append(train_rmse)
    test_mse_list.append(test_mse)
    test_r2_list.append(test_r2)
    test_rmse_list.append(test_rmse)

#Values for drnn
# Evaluation metrics for training set
avg_train_index_of_agreement = np.mean(train_index_of_agreement_list)
avg_train_mse = np.mean(train_mse_list)
avg_train_r2 = np.mean(train_r2_list)
avg_train_rmse = np.mean(train_rmse_list)

print("Average Train Index of Agreement:", round(avg_train_index_of_agreement,2))
print("Average Train MSE:", round(avg_train_mse,2))
print("Average Train R^2:", round(avg_train_r2,2))
print("Average Train RMSE:", round(avg_train_rmse,2))

val_r2 = r2_score(y_val, val_pred)
print('\nValidation R^2:', round(val_r2,2))

avg_test_mse = np.mean(test_mse_list)
avg_test_r2 = np.mean(test_r2_list)
avg_test_rmse = np.mean(test_rmse_list)
avg_test_index_of_agreement = np.mean(test_index_of_agreement_list)


print("\nAverage Test Index of Agreement:", round(avg_test_index_of_agreement,2))
print("Average Test MSE:", round(avg_test_mse,2))
print("Average Test R^2:", round(avg_test_r2,2))
print("Average Test RMSE:", round(avg_test_rmse,2))

"""#GRNN

"""

!pip install pyGRNN

from pyGRNN import GRNN
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
X_val_scaled = scaler_X.transform(X_val)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Define a range of sigma values to try
sigma_values = [0.1, 0.5, 1.0, 2.0]

# Initialize the GRNN model
AGRNN = GRNN(calibration='gradient_search')

# Train GRNN
AGRNN.fit(X_train_scaled, y_train_scaled)

#best sigma value
best_sigma=None
best_val_r2=-float('inf')

for sigma in sigma_values:
    AGRNN.sigma = sigma

    # Predictions
    train_pred_scaled = AGRNN.predict(X_train_scaled)
    test_pred_scaled = AGRNN.predict(X_test_scaled)
    val_pred_scaled = AGRNN.predict(X_val_scaled)

    # Inverse transform predictions to original scale
    train_pred = scaler_y.inverse_transform(train_pred_scaled.reshape(-1, 1)).flatten()
    test_pred = scaler_y.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
    val_pred = scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()

    # Evaluation metrics
    # train_mse = mean_squared_error(y_train, train_pred)
    # train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(y_train, train_pred)

    val_r2 = r2_score(y_val, val_pred)

    # test_mse = mean_squared_error(y_test, test_pred)
    # test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(y_test, test_pred)

    print("Sigma values: ",sigma,"| Validation R^2: ",round(val_r2,2), "|Testing R^2: ", round(test_r2,2))

    #check if this sigma performs better than the current best
    if val_r2 > best_val_r2:
      best_val_r2 = val_r2
      best_sigma = sigma

#use best sigma value
AGRNN.sigma = best_sigma

# Predictions
train_pred_scaled = AGRNN.predict(X_train_scaled)
test_pred_scaled = AGRNN.predict(X_test_scaled)
val_pred_scaled = AGRNN.predict(X_val_scaled)

# Inverse transform predictions to original scale
train_pred = scaler_y.inverse_transform(train_pred_scaled.reshape(-1, 1)).flatten()
test_pred = scaler_y.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
val_pred = scaler_y.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()

# Evaluation metrics
train_mse = mean_squared_error(y_train, train_pred)
train_rmse = np.sqrt(train_mse)
train_r2 = r2_score(y_train, train_pred)

val_r2 = r2_score(y_val, val_pred)

test_mse = mean_squared_error(y_test, test_pred)
test_rmse = np.sqrt(test_mse)
test_r2 = r2_score(y_test, test_pred)

# Calculate Index of Agreement for training set
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train - train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))

# Calculate Index of Agreement for testing set
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

#grnn
print('Training MSE:', round(train_mse,2))
print('Training RMSE:', round(train_rmse,2))
print('Training R^2:', round(train_r2,2))
print('Training Index of Agreement:', round(train_index_of_agreement,2))

print('\nBest alpha value:', best_sigma)

val_r2 = r2_score(y_val, val_pred)
print('Validation r2 value:',round(val_r2,2))

print('\nTesting MSE:', round(test_mse,2))
print('Testing RMSE:', round(test_rmse,2))
print('Testing R^2:', round(test_r2,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))

"""#SVR"""

#SVR
from sklearn.svm import SVR
# Normalize features and target variable using only training data
scaler_X = MinMaxScaler()
scaler_X.fit(X_train)
X_train_scaled = scaler_X.transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test)

scaler_y = MinMaxScaler()
scaler_y.fit(y_train.values.reshape(-1, 1))
y_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1)).ravel()
y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).ravel()

# Parameters

C_arr= [1,0.1,0.01]
eps_arr=[0.1,0.01,0.001]

best_r2_val = -float('inf')

best_C = None
best_epsilon = None


for C in C_arr:
  for epsilon in eps_arr:
    # Train the SVR model
    svr_model = SVR(kernel='rbf', C=C, epsilon=epsilon)
    svr_model.fit(X_train_scaled, y_train_scaled)

    # Predictions on test and val
    svr_test_pred = svr_model.predict(X_test_scaled).ravel()
    svr_val_pred = svr_model.predict(X_val_scaled).ravel()

    #inverse transform
    test_pred=scaler_y.inverse_transform(svr_test_pred.reshape(-1,1)).ravel()
    val_pred=scaler_y.inverse_transform(svr_val_pred.reshape(-1,1)).ravel()

    # Evaluation Metrics on Validation Set and Testing set
    val_r2 = r2_score(y_val,val_pred)
    test_r2 = r2_score(y_test,test_pred)
    print("| C:", C, "|epsilon: ", epsilon, "| Validation R^2:", round(val_r2,2), "| Testing R^2:", round(test_r2,2))

    # Check if this C and epsilon gives better R^2 on validation set
    if val_r2 > best_r2_val:
      best_r2_val = val_r2
      # best_degree=degree
      best_C = C
      best_epsilon = epsilon

# print(" Best degree: ",best_degree)
print(" Best C: ",best_C)
print(" Best epsilon: ",best_epsilon)

#Implement the best model
svr_model = SVR(kernel='rbf',C=best_C,epsilon=best_epsilon)
svr_model.fit(X_train_scaled, y_train_scaled)

# Predictions on train and test and val
svr_train_pred = svr_model.predict(X_train_scaled).ravel()
svr_test_pred = svr_model.predict(X_test_scaled).ravel()
svr_val_pred = svr_model.predict(X_val_scaled).ravel()

#inverse transform
train_pred=scaler_y.inverse_transform(svr_train_pred.reshape(-1,1)).ravel()
test_pred=scaler_y.inverse_transform(svr_test_pred.reshape(-1,1)).ravel()
val_pred=scaler_y.inverse_transform(svr_val_pred.reshape(-1,1)).ravel()
# Evaluation Metrics
#training
train_mse = mean_squared_error(y_train,train_pred)
train_r2 = r2_score(y_train,train_pred)
train_rmse = np.sqrt(train_mse)
mean_observed_train = np.mean(y_train)
train_index_of_agreement = 1 - (np.sum((y_train -train_pred) ** 2) / np.sum((np.abs(y_train - mean_observed_train) + np.abs(train_pred - mean_observed_train)) ** 2))
#val
val_r2=r2_score(y_val,val_pred)
#testing
#Polynomial Regression
#Evaluation on testing data
test_mse = mean_squared_error(y_test,test_pred)
test_r2 = r2_score(y_test,test_pred)
test_rmse = np.sqrt(test_mse)
mean_observed_test = np.mean(y_test)
test_index_of_agreement = 1 - (np.sum((y_test - test_pred) ** 2) / np.sum((np.abs(y_test - mean_observed_test) + np.abs(test_pred - mean_observed_test)) ** 2))

print('\nTesting MSE:',round(test_mse,2))
print('Testing R^2:', round(test_r2,2))
print('Test RMSE:', round(test_rmse,2))
print('Testing Index of Agreement:', round(test_index_of_agreement,2))